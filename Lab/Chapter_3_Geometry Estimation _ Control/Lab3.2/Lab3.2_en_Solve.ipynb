{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f1fb395",
   "metadata": {},
   "source": [
    "# Lab 3.2 — Full Lane-Keeping Pipeline (Perception → Geometry → Control → Overlay)\n",
    "\n",
    "This lab integrates all components built across previous labs:\n",
    "\n",
    "#### From Lab 1 (Perception)\n",
    "- Deep-learning lane mask extraction (YOLO / PIDNet / TwinLite / BiSeNet)\n",
    "\n",
    "\n",
    "#### From Lab 2 (Geometry)\n",
    "- ROI filtering\n",
    "- Morphological refinement\n",
    "- Bird’s-Eye View (BEV) projection\n",
    "\n",
    "#### From Lab 3.1 (Lane Geometry)\n",
    "- center_x() to detect lane center\n",
    "- heading_deg_at_ratio() from two vertical samples\n",
    "- meters_per_pixel() to convert px → meters\n",
    "- Multi-ratio sampling (r = 0.98, 0.92, 0.82, 0.72)\n",
    "- Adaptive ratio selection based on stability\n",
    "\n",
    "\n",
    "### In this Lab you will:\n",
    "1. Implement a simplified steering controller.\n",
    "2. Convert lane geometry → LEFT / RIGHT / STRAIGHT.\n",
    "3. Draw real-time overlay on video frames.\n",
    "4. Run the entire lane-keeping pipeline on a real video input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4441b4f1",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "This lab requires several computer vision and numerical computation libraries.\n",
    "We will install and verify all dependencies before running the geometry and\n",
    "control pipeline.\n",
    "\n",
    "**Required Libraries**\n",
    "- **numpy** — matrix operations  \n",
    "- **opencv-python** — image/video processing  \n",
    "- **matplotlib** — visualization  \n",
    "- **torch**  — load deep learning lane models  \n",
    "- **ultralytics** (optional) — YOLO-based lane/segmentation models  \n",
    "\n",
    "Make sure the runtime has access to GPU if you intend to run deep-learning\n",
    "backends. CPU-only mode is still acceptable for this lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3ae5e07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies installed successfully.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy opencv-python matplotlib ultralytics --quiet\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --quiet\n",
    "\n",
    "print(\"All dependencies installed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b80890",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "We import all standard libraries used throughout the lab:\n",
    "\n",
    "- numpy for numerical computation  \n",
    "- cv2 for image processing  \n",
    "- matplotlib for visualization  \n",
    "- Optional: torch + ultralytics for segmentation backend  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aca940a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cab930",
   "metadata": {},
   "source": [
    "### Version Check\n",
    "\n",
    "It is important to confirm that:\n",
    "- OpenCV is correctly installed  \n",
    "- Torch detects CUDA (optional)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e6692638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV version: 4.12.0\n",
      "Torch version: 2.7.1+cu118\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"OpenCV version:\", cv2.__version__)\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed2cfcc",
   "metadata": {},
   "source": [
    "### 1. Perception Module: YOLOv8 Backend\n",
    "\n",
    "> **Note:** This module was fully implemented in **Lab 1**. In this Lab 3, we will treat it as a **\"Black Box\" sensor** that provides us with the binary lane mask.\n",
    "\n",
    "We utilize the `YoloV8Backend` class to:\n",
    "1.  Load the pre-trained weights (`best.pt`).\n",
    "2.  Perform inference on each video frame.\n",
    "3.  Return a binary segmentation mask (`0` for background, `1` for lane).\n",
    "\n",
    "**Action:** Just **run the cell below** to initialize the backend class. No coding is required here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a9188cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloV8Backend:\n",
    "    \"\"\"\n",
    "    Lightweight wrapper for YOLOv8 segmentation → binary lane mask.\n",
    "    Matches real project structure but simplified for lab environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, weights, device=\"cuda\", imgsz=640, conf=0.18):\n",
    "        self.model = YOLO(weights)\n",
    "        self.device = device\n",
    "        self.imgsz = imgsz\n",
    "        self.conf = conf\n",
    "        self.fp16 = (torch.cuda.is_available() and device != \"cpu\")\n",
    "        self.model.to(device)\n",
    "        self.model.fuse()\n",
    "\n",
    "    def infer_mask01(self, frame_bgr):\n",
    "        H, W = frame_bgr.shape[:2]\n",
    "        res = self.model.predict(frame_bgr, imgsz=self.imgsz, conf=self.conf,\n",
    "                                 device=self.device, verbose=False, half=self.fp16)\n",
    "        r0 = res[0]\n",
    "        if r0.masks is None:\n",
    "            return np.zeros((H, W), np.uint8)\n",
    "\n",
    "        mk = r0.masks.data.cpu().numpy().astype(np.uint8)\n",
    "        merged = np.zeros((H, W), np.uint8)\n",
    "        for k in range(mk.shape[0]):\n",
    "            merged = cv2.bitwise_or(merged, cv2.resize(mk[k], (W, H)))\n",
    "\n",
    "        return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fac23a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8n-seg summary (fused): 85 layers, 3,258,259 parameters, 0 gradients, 11.3 GFLOPs\n",
      "Backend loaded: YOLOv8\n"
     ]
    }
   ],
   "source": [
    "backend = YoloV8Backend(r\"C:\\Users\\admin\\ACE_Finalv4\\AI\\LaneDetection\\Lane_weight\\Yolo_v8\\best.pt\")\n",
    "print(\"Backend loaded:\", backend.name() if hasattr(backend, \"name\") else \"YOLOv8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6827f87",
   "metadata": {},
   "source": [
    "### 2. Preprocessing Module: ROI & BEV (Recap from Lab 2)\n",
    "\n",
    " **Context:** In **Lab 2**, we built a robust preprocessing pipeline to clean the segmentation mask and transform it into Bird's-Eye-View (BEV). We will reuse these utilities here.\n",
    "\n",
    "This module includes:\n",
    "1.  **apply_roi**: Crops the region of interest (removes sky/background).\n",
    "2.  **refine_mask01**: Applies morphological operations (Opening/Closing) to reduce noise and fill gaps.\n",
    "3.  **BEVProjector**: A class that manages the Homography matrix to warp the image from *Perspective View* to *Top-Down View*.\n",
    "\n",
    "**Action:** Run the cell below to define these helper functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea2793a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_roi(mask, poly):\n",
    "    H, W = mask.shape\n",
    "    pts = np.array([(int(x*W), int(y*H)) for x,y in poly], dtype=np.int32)\n",
    "    roi = np.zeros_like(mask)\n",
    "    cv2.fillPoly(roi, [pts], 1)\n",
    "    return mask * roi\n",
    "\n",
    "def refine_mask01(mask):\n",
    "    k = np.ones((5,5), np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, k)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, k)\n",
    "    return mask\n",
    "\n",
    "class BEVProjector:\n",
    "    def __init__(self):\n",
    "        self.src = ((0.20, 0.58),(0.10, 0.90),(0.90, 0.90),(0.80, 0.58))\n",
    "        self.dst = ((0.25, 0.00),(0.25, 1.00),(0.75, 1.00),(0.75, 0.00))\n",
    "        self.M = None\n",
    "        self.M_inv = None\n",
    "        self._wh = None\n",
    "\n",
    "    def _ensure(self, W, H):\n",
    "        if self._wh == (W,H): return\n",
    "        src = np.float32([(x*W,y*H) for x,y in self.src])\n",
    "        dst = np.float32([(x*W,y*H) for x,y in self.dst])\n",
    "        self.M = cv2.getPerspectiveTransform(src, dst)\n",
    "        self.M_inv = cv2.getPerspectiveTransform(dst, src)\n",
    "        self._wh = (W,H)\n",
    "\n",
    "    def warp(self, mask):\n",
    "        H,W = mask.shape\n",
    "        self._ensure(W,H)\n",
    "        bev = cv2.warpPerspective(mask*255, self.M, (W,H), cv2.INTER_NEAREST)\n",
    "        return (bev>0).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c04806",
   "metadata": {},
   "source": [
    "### Geometry Math ( Recap from Lab 3.1)\n",
    "\n",
    "In this section, we re-define all lane-geometry functions required by the full pipeline.  \n",
    "Although these functions were implemented in Lab 3.1, Jupyter notebooks require them to be declared again inside Lab 3.2.\n",
    "\n",
    "This block provides:\n",
    "- center_x() — extracts lane center at a given BEV sampling ratio  \n",
    "- heading_deg_at_ratio() — computes heading angle using two vertical samples  \n",
    "- multi_ratio_measure() — evaluates multiple ratios (0.98, 0.92, 0.82, 0.72)  \n",
    "- adaptive_select_solution() — selects the ratio with the most stable geometry estimate  \n",
    "\n",
    "These functions convert the refined BEV mask into real-valued geometric quantities used by the controller.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "974cb5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_x(bev, r):\n",
    "    H, W = bev.shape\n",
    "    y = int(r * H)\n",
    "    xs = np.where(bev[y] > 0)[0]\n",
    "    if len(xs) < 2:\n",
    "        return np.nan\n",
    "    return 0.5 * (xs[0] + xs[-1])\n",
    "\n",
    "def heading_deg_at_ratio(bev, r, dy=30):\n",
    "    H, W = bev.shape\n",
    "    y = int(r * H)\n",
    "    y2 = max(0, y - dy)\n",
    "\n",
    "    xs1 = np.where(bev[y] > 0)[0]\n",
    "    xs2 = np.where(bev[y2] > 0)[0]\n",
    "\n",
    "    if len(xs1)<2 or len(xs2)<2:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    cx1 = 0.5*(xs1[0]+xs1[-1])\n",
    "    cx2 = 0.5*(xs2[0]+xs2[-1])\n",
    "    dx = cx1 - cx2\n",
    "    dy = (y - y2)\n",
    "\n",
    "    ang = np.degrees(np.arctan2(dx, dy + 1e-6))\n",
    "    return ang, cx1\n",
    "\n",
    "def multi_ratio_measure(bev, ratios, dy_px=30, lane_width_m=0.20):\n",
    "    H, W = bev.shape\n",
    "    mpp = lane_width_m / 20.0  \n",
    "\n",
    "    centers, pos_list, head_list = [], [], []\n",
    "    for r in ratios:\n",
    "        head, cx = heading_deg_at_ratio(bev, r, dy_px)\n",
    "        pos_m = (W/2 - cx) * mpp\n",
    "        centers.append(cx)\n",
    "        pos_list.append(pos_m)\n",
    "        head_list.append(head)\n",
    "\n",
    "    return centers, pos_list, head_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fbae7992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_select(centers, pos_list, head_list, ratios,\n",
    "                             W_STAB=2.0, W_HEAD=0.1, W_LAT=0.05):\n",
    "    best_i = 0\n",
    "    best_score = -1e9\n",
    "\n",
    "    for i,r in enumerate(ratios):\n",
    "        cx = centers[i]\n",
    "        if np.isnan(cx): continue\n",
    "\n",
    "        std_pos = abs(pos_list[i])\n",
    "        mean_head = abs(head_list[i])\n",
    "        score = (W_STAB/(std_pos+1e-6) - W_HEAD*mean_head - W_LAT*(1-r))\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_i = i\n",
    "\n",
    "    return {\n",
    "        \"best_ratio\": ratios[best_i],\n",
    "        \"pos\": pos_list[best_i],\n",
    "        \"head\": head_list[best_i]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23feb638",
   "metadata": {},
   "source": [
    "### Controller\n",
    "\n",
    "The controller converts lane geometry into a steering command used to keep the vehicle centered in the lane.  \n",
    "It combines two types of geometric information:\n",
    "\n",
    "1. **Lateral offset** (pos_m):  \n",
    "   How far the vehicle is from the lane center (in meters).  \n",
    "   - If pos_m > 0 → vehicle is to the RIGHT → must steer LEFT  \n",
    "   - If pos_m < 0 → vehicle is to the LEFT → must steer RIGHT  \n",
    "\n",
    "2. **Heading angle** (head_deg):  \n",
    "   Orientation of the lane relative to the vehicle’s forward direction.  \n",
    "   - Positive angle → lane bends left  \n",
    "   - Negative angle → lane bends right  \n",
    "\n",
    "To correct the vehicle trajectory, we apply a linear control formula that blends these two signals:\n",
    "\n",
    "\n",
    "#### **Control law**\n",
    "\n",
    "$$\n",
    "\\text{steer} = k_{\\text{pos}} \\cdot \\text{pos}_m \\;+\\; k_{\\text{head}} \\cdot \\text{head}_{deg}\n",
    "$$\n",
    "\n",
    "#### Meaning of each term:\n",
    "\n",
    "- $( k_{\\text{pos}} \\cdot \\text{pos}_m $):  \n",
    "  Corrects how far the vehicle is off-center → **primary steering contribution**.\n",
    "\n",
    "- $( k_{\\text{head}} \\cdot \\text{head}_{deg} $):  \n",
    "  Anticipates lane curvature by adjusting steering earlier → **stabilizes turns**.\n",
    "\n",
    "The gains:\n",
    "- $( k_{\\text{pos}} $): how strongly the controller reacts to lateral offset  \n",
    "- $( k_{\\text{head}} $): how strongly it reacts to lane curvature  \n",
    "\n",
    "These values determine how “aggressive” or “smooth” steering is.\n",
    "\n",
    "The final steering output is constrained to stay within hardware limits:\n",
    "\n",
    "$$\n",
    "\\text{steer} \\in [-50, 50]\n",
    "$$\n",
    "\n",
    "\n",
    "#### Steering label (for visualization)\n",
    "\n",
    "To help interpret the numeric steering output, we convert the value into one of three labels:\n",
    "\n",
    "- **LEFT** → steer > threshold  \n",
    "- **RIGHT** → steer < −threshold  \n",
    "- **STRAIGHT** → the steering is small enough to treat as no significant turn  \n",
    "\n",
    "This classification is for visualization only and does not affect control.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58175a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def controller(pos_m, head_deg, k_pos=40.0, k_head=1.5):\n",
    "    steer = k_pos * pos_m + k_head * head_deg\n",
    "    steer = np.clip(steer, -50, 50)\n",
    "    return float(steer)\n",
    "\n",
    "def steering_label(steer, thresh=3.0):\n",
    "    if steer > thresh:\n",
    "        return \"LEFT\"\n",
    "    elif steer < -thresh:\n",
    "        return \"RIGHT\"\n",
    "    else:\n",
    "        return \"STRAIGHT\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7538a85e",
   "metadata": {},
   "source": [
    "### Draw Overlay\n",
    "\n",
    "This function visualizes all lane-geometry outputs on top of the original camera frame.  \n",
    "It provides an intuitive way to understand what the pipeline is doing internally at each step.\n",
    "\n",
    "The overlay includes:\n",
    "\n",
    "#### **1. BEV Mask Projected Back to Camera View**\n",
    "The binary BEV mask (top-down view) is inverse-warped using the inverse homography $( M^{-1} $)  \n",
    "so the detected drivable lane region appears in the camera image.  \n",
    "This helps verify whether the BEV transformation and mask refinement are correct.\n",
    "\n",
    "#### **2. Multi-Ratio Lane Centers**\n",
    "For each sampling ratio (e.g., 0.98 → 0.92 → 0.82 → 0.72):\n",
    "\n",
    "- The center point detected in BEV coordinates is mapped back into camera coordinates.\n",
    "- All ratio points are drawn:\n",
    "  - **Primary ratio** (ratio with best stability score) → RED dot  \n",
    "  - **Other valid ratios** → YELLOW dots  \n",
    "\n",
    "This makes it easy to inspect:\n",
    "- whether lane center detection is stable across ratios  \n",
    "- whether adaptive ratio selection is working correctly  \n",
    "\n",
    "#### **3. On-Screen Debug Information**\n",
    "A small info panel is drawn in the top-left corner showing:\n",
    "\n",
    "- Lateral offset in meters  \n",
    "- Heading angle in degrees  \n",
    "- Steering direction label (“LEFT”, “RIGHT”, “STRAIGHT”)  \n",
    "\n",
    "These values help correlate the visual overlay with the numeric geometry and controller output.\n",
    "\n",
    "#### **Purpose**\n",
    "This visualization step is essential for debugging:\n",
    "- segmentation errors  \n",
    "- BEV misalignment  \n",
    "- geometry instability  \n",
    "- ratio selection  \n",
    "- controller behavior  \n",
    "\n",
    "The function returns an annotated frame that can be displayed or saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "766b7cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_overlay(frame_bgr, bev01, M_inv, ratios, centers_px, primary_idx, pos_m, head_deg, dir_label):\n",
    "    draw = frame_bgr.copy()\n",
    "    H, W = draw.shape[:2]\n",
    "    # ============================\n",
    "    # (1) Warp BEV mask về camera\n",
    "    # ============================\n",
    "    if bev01 is not None and M_inv is not None:\n",
    "        bev_warp = cv2.warpPerspective(\n",
    "            (bev01 * 255).astype(np.uint8),\n",
    "            M_inv, (W, H),\n",
    "            flags=cv2.INTER_NEAREST\n",
    "        )\n",
    "        green = np.zeros_like(draw)\n",
    "        green[bev_warp > 0] = (0, 180, 0)\n",
    "        draw = cv2.addWeighted(draw, 1.0, green, 0.8, 0)\n",
    "\n",
    "    # ============================\n",
    "    # (2) Chuyển điểm BEV → camera\n",
    "    # ============================\n",
    "    pts_bev = []\n",
    "    for r, cx in zip(ratios, centers_px):\n",
    "        if np.isnan(cx):\n",
    "            pts_bev.append([np.nan, np.nan])\n",
    "        else:\n",
    "            y_bev = r * bev01.shape[0]\n",
    "            pts_bev.append([cx, y_bev])\n",
    "\n",
    "    valid_pts = np.array(\n",
    "        [[px, py] for px, py in pts_bev if not np.isnan(px)],\n",
    "        dtype=np.float32\n",
    "    ).reshape(-1, 1, 2)\n",
    "\n",
    "    if valid_pts.shape[0] > 0:\n",
    "        pts_cam = cv2.perspectiveTransform(valid_pts, M_inv)\n",
    "    else:\n",
    "        pts_cam = []\n",
    "\n",
    "    # ============================\n",
    "    # (3) Vẽ các điểm ratio\n",
    "    # ============================\n",
    "    cam_i = 0\n",
    "    for i, (r, cx) in enumerate(zip(ratios, centers_px)):\n",
    "        if np.isnan(cx):\n",
    "            continue\n",
    "        px, py = pts_cam[cam_i][0]\n",
    "        cam_i += 1\n",
    "        if i == primary_idx:\n",
    "            cv2.circle(draw, (int(px), int(py)), 5, (0,0,255), -1)   # đỏ\n",
    "        else:\n",
    "            cv2.circle(draw, (int(px), int(py)), 3, (0,255,255), -1) # vàng\n",
    "\n",
    "    # ============================\n",
    "    # (4) Ô thông góc, hướng\n",
    "    # ============================\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(draw, f\"Pos: {pos_m:+.3f} m\", (15,20), font, 0.45, (0,255,255), 2)\n",
    "    cv2.putText(draw, f\"Head: {head_deg:+.2f} deg\", (15,40), font, 0.55, (0,255,255), 2)\n",
    "    color_dir = (0,255,0) if dir_label==\"STRAIGHT\" else (0,0,255)\n",
    "    cv2.putText(draw, f\"DIR: {dir_label}\", (15,60), font, 0.65, color_dir, 2)\n",
    "    return draw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2349509b",
   "metadata": {},
   "source": [
    "### Full Pipeline Execution (Video Processing)\n",
    "\n",
    "In this final task, you will run the **entire lane-keeping pipeline** on a recorded driving video.\n",
    "This integrates every module developed across the previous labs:\n",
    "\n",
    "- **Perception (Lab 1):** lane mask extraction  \n",
    "- **Geometry (Lab 2):** ROI → refinement → BEV projection  \n",
    "- **Lane estimation (Lab 3.1):** multi-ratio sampling + adaptive selection  \n",
    "- **Control (Lab 3.2):** lateral position + heading → steering command  \n",
    "- **Visualization:** overlaying BEV, sampling points, and steering information  \n",
    "\n",
    "The loop below performs all of these operations frame-by-frame:\n",
    "\n",
    "1. Load a video from disk  \n",
    "2. Apply segmentation and ROI cleaning  \n",
    "3. Project the mask to BEV  \n",
    "4. Compute geometry at multiple ratios  \n",
    "5. Select the most stable ratio  \n",
    "6. Generate a steering command  \n",
    "7. Draw an overlay for debugging  \n",
    "8. Save the processed frames into an output video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8949279",
   "metadata": {},
   "source": [
    "### Step 1 — Load Input Video\n",
    "\n",
    "In this step, we specify the path to the driving video that will be processed by the full lane-keeping pipeline.  \n",
    "The cell includes:\n",
    "\n",
    "- a user-editable `video_path`\n",
    "- automatic file existence checking\n",
    "- a fallback input prompt if the file is missing\n",
    "- initialization of the OpenCV video capture object\n",
    "\n",
    "This ensures that the pipeline always starts with a valid input video and provides clear feedback to the learner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9121a7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded input video: C:\\Users\\admin\\ACE_images\\esp32_capture.mp4\n"
     ]
    }
   ],
   "source": [
    "video_path = r\"C:\\Users\\admin\\ACE_images\\esp32_capture.mp4\"\n",
    "# Optional fallback if file is missing\n",
    "if not os.path.exists(video_path):\n",
    "    print(f\"[INFO] The path '{video_path}' does not exist.\")\n",
    "    print(\"Please enter a valid video file path:\")\n",
    "    video_path = input(\"Video path: \").strip()\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    raise FileNotFoundError(f\"Could not open video: {video_path}\")\n",
    "\n",
    "print(f\"[OK] Loaded input video: {video_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2138ab",
   "metadata": {},
   "source": [
    "### Step 2 — Configure Output Video Writer\n",
    "\n",
    "We now configure the video writer that will save the processed frames produced by the pipeline.\n",
    "\n",
    "This cell performs:\n",
    "- extraction of resolution and FPS from the input video  \n",
    "- creation of an MP4 writer using OpenCV  \n",
    "- confirmation messages showing output location and video parameters  \n",
    "\n",
    "The processed visualization frames will be written to `lab3_output.mp4`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5366ebb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Output will be saved to: lab3_output.mp4\n",
      "[INFO] Resolution: 320x240, FPS: 25.0\n"
     ]
    }
   ],
   "source": [
    "frame_width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps_in       = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "\n",
    "output_path = \"lab3_output.mp4\"\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out = cv2.VideoWriter(\n",
    "    output_path,\n",
    "    fourcc,\n",
    "    fps_in,\n",
    "    (frame_width, frame_height)\n",
    ")\n",
    "\n",
    "print(f\"[OK] Output will be saved to: {output_path}\")\n",
    "print(f\"[INFO] Resolution: {frame_width}x{frame_height}, FPS: {fps_in}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d4224",
   "metadata": {},
   "source": [
    "### Step 3 — Run the Full Lane-Keeping Pipeline\n",
    "\n",
    "This cell integrates all components developed across the previous labs and runs them frame-by-frame on the input video.\n",
    "\n",
    "The pipeline consists of:\n",
    "\n",
    "1. ***Segmentation (Lab 1)*** \n",
    "   Generate a lane mask from each input frame and refine it using ROI + morphology.\n",
    "\n",
    "2. ***BEV Projection (Lab 2)***\n",
    "   Convert the lane mask to a bird’s-eye-view representation for easier geometry extraction.\n",
    "\n",
    "3. ***Multi-Ratio Geometry Sampling (Lab 3.1)***\n",
    "   Estimate lane center and heading at multiple vertical sampling ratios.\n",
    "\n",
    "4. ***Adaptive Ratio Selection (Lab 3.1)*** \n",
    "   Automatically choose the most stable geometry among the sampled ratios.\n",
    "\n",
    "5. ***Controller Computation (Lab 3.2)***\n",
    "   Convert lane geometry into a steering command and direction label.\n",
    "\n",
    "6. ***Visualization Overlay*** \n",
    "   Render the BEV projection, ratio points, and steering debug information onto the frame.\n",
    "\n",
    "7. ***Video Export***\n",
    "   Write the processed frame to the output video.\n",
    "\n",
    "When the loop finishes, the final processed video is saved as `lab3_output.mp4`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "076fb53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE — saved to lab3_output.mp4\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Lane-Keeping Full Pipeline (End-to-End)\n",
    "# ======================================================\n",
    "\n",
    "proj = BEVProjector()\n",
    "ratios = [0.98, 0.92, 0.82, 0.72]\n",
    "ROI = np.float32([[0.03,0.58],[0.97,0.58],[0.97,0.99],[0.03,0.99]])\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # (1) Segmentation + ROI refinement\n",
    "    # --------------------------------------------------\n",
    "    mask = backend.infer_mask01(frame)\n",
    "    mask = apply_roi(mask, ROI)\n",
    "    mask = refine_mask01(mask)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # (2) BEV projection\n",
    "    # --------------------------------------------------\n",
    "    bev = proj.warp(mask)\n",
    "    bev01 = bev   # alias\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # (3) Multi-ratio geometry sampling\n",
    "    # --------------------------------------------------\n",
    "    centers, pos_list, head_list = multi_ratio_measure(bev, ratios)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # (4) Adaptive ratio selection\n",
    "    # --------------------------------------------------\n",
    "    sel = adaptive_select(centers, pos_list, head_list, ratios)\n",
    "    pos    = sel[\"pos\"]\n",
    "    head   = sel[\"head\"]\n",
    "    r_star = sel[\"best_ratio\"]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # (5) Steering computation\n",
    "    # --------------------------------------------------\n",
    "    steer = controller(pos, head)\n",
    "    label = steering_label(steer)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # (6) Ratio → pixel conversion for visualization\n",
    "    # --------------------------------------------------\n",
    "    H = bev.shape[0]\n",
    "    cy = int(r_star * H)\n",
    "    cx = center_x(bev, r_star)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # (7) Overlay visualization\n",
    "    # --------------------------------------------------\n",
    "    overlay = draw_overlay(\n",
    "        frame_bgr   = frame,\n",
    "        bev01       = bev,\n",
    "        M_inv       = proj.M_inv,\n",
    "        ratios      = ratios,\n",
    "        centers_px  = centers,\n",
    "        primary_idx = ratios.index(r_star),\n",
    "        pos_m       = pos,\n",
    "        head_deg    = head,\n",
    "        dir_label   = label\n",
    "    )\n",
    "\n",
    "    out.write(overlay)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"DONE — saved to lab3_output.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26adc633",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
